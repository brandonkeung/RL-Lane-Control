{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4a9722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b55bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.actions.clear()\n",
    "        self.states.clear()\n",
    "        self.logprobs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.state_values.clear()\n",
    "        self.is_terminals.clear()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def act(self, state, deterministic=False):\n",
    "        action_probs = self.actor(state)\n",
    "\n",
    "        if deterministic:\n",
    "            action = torch.argmax(action_probs, dim=-1)\n",
    "            action_logprob = torch.log(action_probs.gather(1, action.unsqueeze(-1)).squeeze(-1))\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            action_logprob = dist.log_prob(action)\n",
    "\n",
    "        state_value = self.critic(state)\n",
    "        return action, action_logprob, state_value\n",
    "\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        action_probs = self.actor(states)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(actions)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(states).squeeze(-1)\n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).flatten().unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, action_logprob, state_value = self.policy_old.act(state, deterministic=deterministic)\n",
    "\n",
    "        if not deterministic:\n",
    "            self.buffer.states.append(state.squeeze(0))\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_value)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward * (1 - is_terminal))\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        old_states = torch.stack(self.buffer.states).to(device)\n",
    "        old_actions = torch.stack(self.buffer.actions).to(device)\n",
    "        old_logprobs = torch.stack(self.buffer.logprobs).to(device)\n",
    "        old_state_values = torch.stack(self.buffer.state_values).to(device)\n",
    "\n",
    "        advantages = rewards - old_state_values.detach()\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.buffer.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8cfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"observation\":       {\"type\": \"Kinematics\"},\n",
    "    \"action\":            {\"type\": \"DiscreteMetaAction\"},\n",
    "    \"lanes_count\":       4,\n",
    "    \"vehicles_count\":    50,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"duration\":          40,\n",
    "    \"ego_spacing\":       2,\n",
    "    \"vehicles_density\":  1,\n",
    "    \"collision_reward\":  -1,\n",
    "    \"right_lane_reward\": 0.1,\n",
    "    \"high_speed_reward\": 0.4,\n",
    "    \"lane_change_reward\": 0,\n",
    "    \"normalize_reward\":  True,\n",
    "    \"offroad_terminal\":  False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ac729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (5, 5), float32)\n",
      "Episode 10, Reward: 7.19, Running Avg (50): 8.32\n",
      "Episode 20, Reward: 2.62, Running Avg (50): 8.76\n",
      "Episode 30, Reward: 29.95, Running Avg (50): 10.92\n",
      "Episode 40, Reward: 3.26, Running Avg (50): 10.65\n",
      "Episode 50, Reward: 3.18, Running Avg (50): 11.05\n",
      "Episode 60, Reward: 1.69, Running Avg (50): 10.85\n",
      "Episode 70, Reward: 3.91, Running Avg (50): 10.91\n",
      "Episode 80, Reward: 15.37, Running Avg (50): 10.10\n",
      "Episode 90, Reward: 8.26, Running Avg (50): 10.21\n",
      "Episode 100, Reward: 16.49, Running Avg (50): 10.34\n",
      "Episode 110, Reward: 6.26, Running Avg (50): 11.04\n",
      "Episode 120, Reward: 30.21, Running Avg (50): 10.92\n",
      "Episode 130, Reward: 18.12, Running Avg (50): 10.10\n",
      "Episode 140, Reward: 2.71, Running Avg (50): 9.61\n",
      "Episode 150, Reward: 6.78, Running Avg (50): 9.14\n",
      "Episode 160, Reward: 30.01, Running Avg (50): 9.18\n",
      "Episode 170, Reward: 7.39, Running Avg (50): 8.93\n",
      "Episode 180, Reward: 17.10, Running Avg (50): 10.18\n",
      "Episode 190, Reward: 6.08, Running Avg (50): 10.91\n",
      "Episode 200, Reward: 9.70, Running Avg (50): 10.18\n",
      "Episode 210, Reward: 17.25, Running Avg (50): 10.90\n",
      "Episode 220, Reward: 28.77, Running Avg (50): 11.41\n",
      "Episode 230, Reward: 2.98, Running Avg (50): 10.79\n",
      "Episode 240, Reward: 30.45, Running Avg (50): 10.47\n",
      "Episode 250, Reward: 5.38, Running Avg (50): 10.98\n",
      "Episode 260, Reward: 5.31, Running Avg (50): 10.62\n",
      "Episode 270, Reward: 1.73, Running Avg (50): 10.71\n",
      "Episode 280, Reward: 19.77, Running Avg (50): 10.53\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    env_name = \"highway-v0\"\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\").unwrapped\n",
    "    env.configure(BASE_CONFIG)\n",
    "    print(env.observation_space)\n",
    "\n",
    "    state_dim = np.prod(env.observation_space.shape)\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    max_episodes = 1000\n",
    "    max_ep_len = 1000\n",
    "    update_timestep = 4000\n",
    "    K_epochs = 80\n",
    "    eps_clip = 0.2\n",
    "    gamma = 0.99\n",
    "    lr_actor = 0.0003\n",
    "    lr_critic = 0.001\n",
    "\n",
    "    episode_reward_history = []\n",
    "    running_rewards = []\n",
    "    last_n_reward = 50 \n",
    "    \n",
    "    \n",
    "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    \n",
    "\n",
    "    while i_episode < max_episodes:\n",
    "        obs, _ = env.reset()\n",
    "        current_ep_reward = 0\n",
    "\n",
    "        for _ in range(max_ep_len):\n",
    "            action = ppo_agent.select_action(obs)\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done or truncated)\n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo_agent.update()\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        i_episode += 1\n",
    "        episode_reward_history.append(current_ep_reward)\n",
    "\n",
    "        # Compute running average\n",
    "        if len(episode_reward_history) > last_n_reward:\n",
    "            running_reward = np.mean(episode_reward_history[-last_n_reward:])\n",
    "        else:\n",
    "            running_reward = np.mean(episode_reward_history)\n",
    "        running_rewards.append(running_reward)\n",
    "        if i_episode % 10 == 0:\n",
    "            print(f\"Episode {i_episode}, Reward: {current_ep_reward:.2f}, Running Avg ({last_n_reward}): {running_reward:.2f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(episode_reward_history, label=\"Episode Reward\")\n",
    "    plt.plot(running_rewards, label=f\"Running Avg (last {last_n_reward})\", linewidth=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"PPO Training Rewards\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0227a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent for behavior: Safe\n",
      "\n",
      "Training scenario: Safe\n",
      "Episode 10, Reward: 12.36, Running Avg (50): 12.96\n",
      "Episode 20, Reward: 19.51, Running Avg (50): 14.20\n",
      "Episode 30, Reward: 29.87, Running Avg (50): 14.79\n",
      "Episode 40, Reward: 11.64, Running Avg (50): 13.56\n",
      "Episode 50, Reward: 35.59, Running Avg (50): 14.26\n",
      "Episode 60, Reward: 3.03, Running Avg (50): 12.63\n",
      "Episode 70, Reward: 7.90, Running Avg (50): 12.34\n",
      "Episode 80, Reward: 15.69, Running Avg (50): 11.23\n",
      "Episode 90, Reward: 4.87, Running Avg (50): 10.41\n",
      "Episode 100, Reward: 26.50, Running Avg (50): 10.88\n",
      "Episode 110, Reward: 20.74, Running Avg (50): 12.54\n",
      "Episode 120, Reward: 3.97, Running Avg (50): 12.55\n",
      "Episode 130, Reward: 31.17, Running Avg (50): 12.63\n",
      "Episode 140, Reward: 2.95, Running Avg (50): 13.45\n",
      "Episode 150, Reward: 2.86, Running Avg (50): 12.11\n",
      "Episode 160, Reward: 9.81, Running Avg (50): 12.55\n",
      "Episode 170, Reward: 4.95, Running Avg (50): 13.29\n",
      "Episode 180, Reward: 13.16, Running Avg (50): 12.70\n",
      "Episode 190, Reward: 8.61, Running Avg (50): 13.96\n",
      "Episode 200, Reward: 2.04, Running Avg (50): 14.74\n",
      "Episode 210, Reward: 17.00, Running Avg (50): 13.87\n",
      "Episode 220, Reward: 23.30, Running Avg (50): 13.19\n",
      "Episode 230, Reward: 12.84, Running Avg (50): 14.38\n",
      "Episode 240, Reward: 11.46, Running Avg (50): 13.88\n",
      "Episode 250, Reward: 18.95, Running Avg (50): 13.08\n",
      "Episode 260, Reward: 7.78, Running Avg (50): 13.54\n",
      "Episode 270, Reward: 13.72, Running Avg (50): 13.23\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 238.42 GiB. GPU 0 has a total capacity of 8.00 GiB of which 6.50 GiB is free. Of the allocated memory 405.54 MiB is allocated by PyTorch, and 14.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m cfg.update(behavior_cfg) \n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining agent for behavior: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m agent = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m torch.cuda.empty_cache()\n\u001b[32m    112\u001b[39m trained_agents[label] = agent\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(cfg, label)\u001b[39m\n\u001b[32m     65\u001b[39m current_ep_reward += reward\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_step % update_timestep == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43mppo_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m obs = next_obs\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mPPO.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m logprobs, state_values, dist_entropy = \u001b[38;5;28mself\u001b[39m.policy.evaluate(old_states, old_actions)\n\u001b[32m    121\u001b[39m ratios = torch.exp(logprobs - old_logprobs.detach())\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m surr1 = \u001b[43mratios\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\n\u001b[32m    124\u001b[39m surr2 = torch.clamp(ratios, \u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.eps_clip, \u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.eps_clip) * advantages\n\u001b[32m    126\u001b[39m loss = -torch.min(surr1, surr2) + \u001b[32m0.5\u001b[39m * \u001b[38;5;28mself\u001b[39m.MseLoss(state_values, rewards) - \u001b[32m0.01\u001b[39m * dist_entropy\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 238.42 GiB. GPU 0 has a total capacity of 8.00 GiB of which 6.50 GiB is free. Of the allocated memory 405.54 MiB is allocated by PyTorch, and 14.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "DRIVER_BEHAVIORS = {\n",
    "    \"Safe\": {\n",
    "        \"reward_speed_range\": [15, 25],\n",
    "        \"collision_reward\": -5.0,\n",
    "        \"lane_change_reward\": -0.2,\n",
    "        \"right_lane_reward\": 0.3,\n",
    "        \"vehicles_density\": 1,\n",
    "    },\n",
    "    \"Normal\": {\n",
    "        \"reward_speed_range\": [20, 30],\n",
    "        \"collision_reward\": -2.0,\n",
    "        \"lane_change_reward\": 0.0,\n",
    "        \"right_lane_reward\": 0.1,\n",
    "        \"vehicles_density\": 1,\n",
    "    },\n",
    "    \"Aggressive\": {\n",
    "        \"reward_speed_range\": [30, 40],\n",
    "        \"collision_reward\": -0.5,\n",
    "        \"lane_change_reward\": +0.3,\n",
    "        \"right_lane_reward\": -0.1,\n",
    "        \"vehicles_density\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "def train(cfg, label=\"\"):\n",
    "    env = gym.make(\"highway-v0\", render_mode=\"rgb_array\").unwrapped\n",
    "    env.configure(cfg)\n",
    "    print(f\"\\nTraining scenario: {label}\")\n",
    "\n",
    "    state_dim = np.prod(env.observation_space.shape)\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # PPO hyperparameters\n",
    "    max_episodes = 1500\n",
    "    max_ep_len = 1000\n",
    "    update_timestep = 4000\n",
    "    K_epochs = 80\n",
    "    eps_clip = 0.2\n",
    "    gamma = 0.99\n",
    "    lr_actor = 0.0003\n",
    "    lr_critic = 0.001\n",
    "\n",
    "    # Reward tracking\n",
    "    episode_reward_history = []\n",
    "    running_rewards = []\n",
    "    last_n_reward = 50\n",
    "\n",
    "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "\n",
    "    while i_episode < max_episodes:\n",
    "        obs, _ = env.reset()\n",
    "        current_ep_reward = 0\n",
    "\n",
    "        for _ in range(max_ep_len):\n",
    "            action = ppo_agent.select_action(obs)\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done or truncated)\n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo_agent.update()\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        i_episode += 1\n",
    "        episode_reward_history.append(current_ep_reward)\n",
    "\n",
    "        running_reward = (\n",
    "            np.mean(episode_reward_history[-last_n_reward:])\n",
    "            if len(episode_reward_history) > last_n_reward\n",
    "            else np.mean(episode_reward_history)\n",
    "        )\n",
    "        running_rewards.append(running_reward)\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print(f\"Episode {i_episode}, Reward: {current_ep_reward:.2f}, Running Avg ({last_n_reward}): {running_reward:.2f}\")\n",
    "\n",
    "    # Plot reward history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(episode_reward_history, label=\"Episode Reward\")\n",
    "    plt.plot(running_rewards, label=f\"Running Avg (last {last_n_reward})\", linewidth=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"PPO Training Rewards – {label}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    env.close()\n",
    "    return ppo_agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agents = {}\n",
    "    for label, behavior_cfg in DRIVER_BEHAVIORS.items():\n",
    "        cfg = BASE_CONFIG.copy()\n",
    "        cfg.update(behavior_cfg) \n",
    "        print(f\"\\nTraining agent for behavior: {label}\")\n",
    "        agent = train(cfg, label)\n",
    "        torch.cuda.empty_cache()\n",
    "        trained_agents[label] = agent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2cae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(env, model, episodes=50, max_steps=1000, is_ppo=True):\n",
    "    rewards = []\n",
    "    collisions = []\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0.0\n",
    "        collision_count = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            if is_ppo:\n",
    "                action = model.select_action(state, deterministic=True)\n",
    "            else:\n",
    "                q_vals = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "                action = int(np.argmax(q_vals))\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            total_reward += reward\n",
    "            if info.get(\"crashed\", False) or reward < 0:\n",
    "                collision_count += 1\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        collisions.append(collision_count)\n",
    "\n",
    "    avg_collisions = np.mean(collisions)\n",
    "    return rewards, avg_collisions\n",
    "\n",
    "def evaluate_all_agents(trained_agents, driver_configs, base_config, episodes=50, is_ppo=True):\n",
    "    results = []\n",
    "\n",
    "    for label, agent in trained_agents.items():\n",
    "        print(f\"\\nEvaluating agent: {label}\")\n",
    "\n",
    "        cfg = base_config.copy()\n",
    "        cfg.update(driver_configs[label])\n",
    "        env = gym.make(\"highway-v0\", render_mode=None).unwrapped\n",
    "        env.configure(cfg)\n",
    "\n",
    "        rewards, avg_collisions = evaluate_model(env, agent, episodes=episodes, is_ppo=is_ppo)\n",
    "\n",
    "        results.append({\n",
    "            \"behavior\": label,\n",
    "            \"mean_reward\": np.mean(rewards),\n",
    "            \"std_reward\": np.std(rewards),\n",
    "            \"avg_collisions\": avg_collisions\n",
    "        })\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(df.to_string(index=False))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating agent: Safe\n",
      "\n",
      "Evaluating agent: Normal\n",
      "\n",
      "Evaluating agent: Aggressive\n",
      "\n",
      "Evaluation Results:\n",
      "  behavior  mean_reward  std_reward  avg_collisions\n",
      "      Safe    37.200214    0.701754             0.0\n",
      "    Normal    32.704489    1.160953             0.1\n",
      "Aggressive     2.925000    2.616441             1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>behavior</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>avg_collisions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Safe</td>\n",
       "      <td>37.200214</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal</td>\n",
       "      <td>32.704489</td>\n",
       "      <td>1.160953</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aggressive</td>\n",
       "      <td>2.925000</td>\n",
       "      <td>2.616441</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     behavior  mean_reward  std_reward  avg_collisions\n",
       "0        Safe    37.200214    0.701754             0.0\n",
       "1      Normal    32.704489    1.160953             0.1\n",
       "2  Aggressive     2.925000    2.616441             1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = evaluate_all_agents(trained_agents, DRIVER_BEHAVIORS, BASE_CONFIG, episodes=50, is_ppo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b9cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8406ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
