{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import highway_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "NUM_EPISODES       = 500\n",
    "MAX_STEPS_PER_EP   = 200\n",
    "BATCH_SIZE         = 64\n",
    "BUFFER_SIZE        = 50_000\n",
    "GAMMA              = 0.99\n",
    "EPS_START, EPS_END = 1.0, 0.01\n",
    "EPS_DECAY          = 0.99\n",
    "TARGET_UPD_FREQ    = 1_000      # steps\n",
    "LR                 = 5e-4\n",
    "\n",
    "# --- BASE CONFIG & VARIANTS ---\n",
    "BASE_CONFIG = {\n",
    "    \"observation\":       {\"type\": \"Kinematics\"},\n",
    "    \"action\":            {\"type\": \"DiscreteMetaAction\"},\n",
    "    \"lanes_count\":       4,\n",
    "    \"vehicles_count\":    50,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"duration\":          40,\n",
    "    \"ego_spacing\":       2,\n",
    "    \"vehicles_density\":  1,\n",
    "    \"collision_reward\":  -1,\n",
    "    \"right_lane_reward\": 0.1,\n",
    "    \"high_speed_reward\": 0.4,        # keep constant\n",
    "    \"lane_change_reward\": 0,\n",
    "    \"normalize_reward\":  True,\n",
    "    \"offroad_terminal\":  False,\n",
    "}\n",
    "\n",
    "SCENARIOS = {\n",
    "    \"Slow\":   {\"reward_speed_range\": [10, 20]},\n",
    "    \"Normal\": {\"reward_speed_range\": [20, 30]},\n",
    "    \"Fast\":   {\"reward_speed_range\": [30, 40]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model(input_shape, num_actions):\n",
    "    \"\"\"Simple MLP Q-network.\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(num_actions, activation=\"linear\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "def train_on_env(env_config, label):\n",
    "    \"\"\"Train a fresh DQN, recording per‐episode total rewards.\"\"\"\n",
    "    # 1) build env\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg.update(env_config)\n",
    "    env = gym.make(\"highway-v0\", render_mode=\"rgb_array\", config=cfg)\n",
    "    obs_shape   = env.observation_space.shape\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # 2) networks, buffer, optimizer\n",
    "    q_model  = create_q_model(obs_shape, num_actions)\n",
    "    q_target = create_q_model(obs_shape, num_actions)\n",
    "    q_target.set_weights(q_model.get_weights())\n",
    "    buffer   = deque(maxlen=BUFFER_SIZE)\n",
    "    optimizer = tf.keras.optimizers.Adam(LR)\n",
    "    loss_fn   = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    epsilon    = EPS_START\n",
    "    step_count = 0\n",
    "    episode_rewards = []  # <-- track total reward per episode\n",
    "\n",
    "    # 3) training loop\n",
    "    for ep in range(1, NUM_EPISODES + 1):\n",
    "        state, _ = env.reset(seed=ep)\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for t in range(MAX_STEPS_PER_EP):\n",
    "            # ε-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_vals = q_model.predict(state[np.newaxis], verbose=0)[0]\n",
    "                action = int(np.argmax(q_vals))\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # learning update\n",
    "            if len(buffer) >= BATCH_SIZE:\n",
    "                batch = random.sample(buffer, BATCH_SIZE)\n",
    "                S  = np.array([b[0] for b in batch])\n",
    "                A  = np.array([b[1] for b in batch])\n",
    "                R  = np.array([b[2] for b in batch], dtype=float)\n",
    "                S2 = np.array([b[3] for b in batch])\n",
    "                D  = np.array([b[4] for b in batch], dtype=float)\n",
    "\n",
    "                q_next = q_target.predict(S2, verbose=0)\n",
    "                max_q  = np.max(q_next, axis=1)\n",
    "                y      = R + (1 - D) * GAMMA * max_q\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_pred = q_model(S)\n",
    "                    q_sel  = tf.reduce_sum(q_pred * tf.one_hot(A, num_actions), axis=1)\n",
    "                    loss   = loss_fn(y, q_sel)\n",
    "                grads = tape.gradient(loss, q_model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, q_model.trainable_variables))\n",
    "\n",
    "            # periodically sync target network\n",
    "            if step_count % TARGET_UPD_FREQ == 0:\n",
    "                q_target.set_weights(q_model.get_weights())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # record episode reward and decay ε\n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"[{label}] Episode {ep}/{NUM_EPISODES} — Reward: {total_reward:.1f}, ε: {epsilon:.3f}\")\n",
    "\n",
    "    return env, q_model, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fast': {'reward_speed_range': [30, 40]},\n",
      " 'Normal': {'reward_speed_range': [20, 30]},\n",
      " 'Slow': {'reward_speed_range': [10, 20]}}\n",
      "\n",
      "========================================\n",
      " Training scenario ▶️ Slow\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonkeung/.pyenv/versions/3.11.9/envs/RL2/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Slow] Episode 10/500 — Reward: 2.2, ε: 0.904\n",
      "[Slow] Episode 20/500 — Reward: 7.2, ε: 0.818\n",
      "[Slow] Episode 30/500 — Reward: 3.8, ε: 0.740\n",
      "[Slow] Episode 40/500 — Reward: 21.6, ε: 0.669\n",
      "[Slow] Episode 50/500 — Reward: 38.0, ε: 0.605\n",
      "[Slow] Episode 60/500 — Reward: 19.8, ε: 0.547\n",
      "[Slow] Episode 70/500 — Reward: 5.3, ε: 0.495\n",
      "[Slow] Episode 80/500 — Reward: 13.3, ε: 0.448\n",
      "[Slow] Episode 90/500 — Reward: 11.1, ε: 0.405\n",
      "[Slow] Episode 100/500 — Reward: 27.4, ε: 0.366\n",
      "[Slow] Episode 110/500 — Reward: 16.2, ε: 0.331\n",
      "[Slow] Episode 120/500 — Reward: 37.9, ε: 0.299\n",
      "[Slow] Episode 130/500 — Reward: 2.3, ε: 0.271\n",
      "[Slow] Episode 140/500 — Reward: 39.7, ε: 0.245\n",
      "[Slow] Episode 150/500 — Reward: 39.0, ε: 0.221\n",
      "[Slow] Episode 160/500 — Reward: 37.9, ε: 0.200\n",
      "[Slow] Episode 170/500 — Reward: 37.4, ε: 0.181\n",
      "[Slow] Episode 180/500 — Reward: 37.7, ε: 0.164\n",
      "[Slow] Episode 190/500 — Reward: 38.2, ε: 0.148\n",
      "[Slow] Episode 200/500 — Reward: 38.3, ε: 0.134\n",
      "[Slow] Episode 210/500 — Reward: 39.0, ε: 0.121\n",
      "[Slow] Episode 220/500 — Reward: 39.0, ε: 0.110\n",
      "[Slow] Episode 230/500 — Reward: 40.0, ε: 0.099\n",
      "[Slow] Episode 240/500 — Reward: 39.8, ε: 0.090\n",
      "[Slow] Episode 250/500 — Reward: 39.9, ε: 0.081\n",
      "[Slow] Episode 260/500 — Reward: 39.9, ε: 0.073\n",
      "[Slow] Episode 270/500 — Reward: 40.0, ε: 0.066\n",
      "[Slow] Episode 280/500 — Reward: 39.0, ε: 0.060\n",
      "[Slow] Episode 290/500 — Reward: 37.3, ε: 0.054\n",
      "[Slow] Episode 300/500 — Reward: 37.6, ε: 0.049\n",
      "[Slow] Episode 310/500 — Reward: 34.2, ε: 0.044\n",
      "[Slow] Episode 320/500 — Reward: 39.0, ε: 0.040\n",
      "[Slow] Episode 330/500 — Reward: 37.8, ε: 0.036\n",
      "[Slow] Episode 340/500 — Reward: 39.0, ε: 0.033\n",
      "[Slow] Episode 350/500 — Reward: 38.2, ε: 0.030\n",
      "[Slow] Episode 360/500 — Reward: 39.0, ε: 0.027\n",
      "[Slow] Episode 370/500 — Reward: 37.5, ε: 0.024\n",
      "[Slow] Episode 380/500 — Reward: 40.0, ε: 0.022\n",
      "[Slow] Episode 390/500 — Reward: 38.0, ε: 0.020\n",
      "[Slow] Episode 400/500 — Reward: 40.0, ε: 0.018\n",
      "[Slow] Episode 410/500 — Reward: 39.8, ε: 0.016\n",
      "[Slow] Episode 420/500 — Reward: 38.1, ε: 0.015\n",
      "[Slow] Episode 430/500 — Reward: 38.4, ε: 0.013\n",
      "[Slow] Episode 440/500 — Reward: 38.2, ε: 0.012\n",
      "[Slow] Episode 450/500 — Reward: 38.2, ε: 0.011\n",
      "[Slow] Episode 460/500 — Reward: 39.2, ε: 0.010\n",
      "[Slow] Episode 470/500 — Reward: 39.4, ε: 0.010\n",
      "[Slow] Episode 480/500 — Reward: 39.0, ε: 0.010\n",
      "[Slow] Episode 490/500 — Reward: 38.6, ε: 0.010\n",
      "[Slow] Episode 500/500 — Reward: 38.2, ε: 0.010\n",
      "\n",
      "========================================\n",
      " Training scenario ▶️ Normal\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonkeung/.pyenv/versions/3.11.9/envs/RL2/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normal] Episode 10/500 — Reward: 14.7, ε: 0.904\n",
      "[Normal] Episode 20/500 — Reward: 12.8, ε: 0.818\n",
      "[Normal] Episode 30/500 — Reward: 10.4, ε: 0.740\n",
      "[Normal] Episode 40/500 — Reward: 10.1, ε: 0.669\n",
      "[Normal] Episode 50/500 — Reward: 2.1, ε: 0.605\n",
      "[Normal] Episode 60/500 — Reward: 21.3, ε: 0.547\n",
      "[Normal] Episode 70/500 — Reward: 4.8, ε: 0.495\n",
      "[Normal] Episode 80/500 — Reward: 12.8, ε: 0.448\n",
      "[Normal] Episode 90/500 — Reward: 9.5, ε: 0.405\n",
      "[Normal] Episode 100/500 — Reward: 9.7, ε: 0.366\n",
      "[Normal] Episode 110/500 — Reward: 19.6, ε: 0.331\n",
      "[Normal] Episode 120/500 — Reward: 30.5, ε: 0.299\n",
      "[Normal] Episode 130/500 — Reward: 7.7, ε: 0.271\n",
      "[Normal] Episode 140/500 — Reward: 28.7, ε: 0.245\n",
      "[Normal] Episode 150/500 — Reward: 10.9, ε: 0.221\n",
      "[Normal] Episode 160/500 — Reward: 9.5, ε: 0.200\n",
      "[Normal] Episode 170/500 — Reward: 3.3, ε: 0.181\n",
      "[Normal] Episode 180/500 — Reward: 17.0, ε: 0.164\n",
      "[Normal] Episode 190/500 — Reward: 13.5, ε: 0.148\n",
      "[Normal] Episode 200/500 — Reward: 10.2, ε: 0.134\n",
      "[Normal] Episode 210/500 — Reward: 3.8, ε: 0.121\n",
      "[Normal] Episode 220/500 — Reward: 2.0, ε: 0.110\n",
      "[Normal] Episode 230/500 — Reward: 7.9, ε: 0.099\n",
      "[Normal] Episode 240/500 — Reward: 17.7, ε: 0.090\n",
      "[Normal] Episode 250/500 — Reward: 10.2, ε: 0.081\n",
      "[Normal] Episode 260/500 — Reward: 19.8, ε: 0.073\n",
      "[Normal] Episode 270/500 — Reward: 8.3, ε: 0.066\n",
      "[Normal] Episode 280/500 — Reward: 11.0, ε: 0.060\n",
      "[Normal] Episode 290/500 — Reward: 15.2, ε: 0.054\n",
      "[Normal] Episode 300/500 — Reward: 18.5, ε: 0.049\n",
      "[Normal] Episode 310/500 — Reward: 15.0, ε: 0.044\n",
      "[Normal] Episode 320/500 — Reward: 6.9, ε: 0.040\n",
      "[Normal] Episode 330/500 — Reward: 23.6, ε: 0.036\n",
      "[Normal] Episode 340/500 — Reward: 5.5, ε: 0.033\n",
      "[Normal] Episode 350/500 — Reward: 3.8, ε: 0.030\n",
      "[Normal] Episode 360/500 — Reward: 14.3, ε: 0.027\n",
      "[Normal] Episode 370/500 — Reward: 27.8, ε: 0.024\n",
      "[Normal] Episode 380/500 — Reward: 31.3, ε: 0.022\n",
      "[Normal] Episode 390/500 — Reward: 30.8, ε: 0.020\n",
      "[Normal] Episode 400/500 — Reward: 33.1, ε: 0.018\n",
      "[Normal] Episode 410/500 — Reward: 15.5, ε: 0.016\n",
      "[Normal] Episode 420/500 — Reward: 6.3, ε: 0.015\n",
      "[Normal] Episode 430/500 — Reward: 26.9, ε: 0.013\n",
      "[Normal] Episode 440/500 — Reward: 28.7, ε: 0.012\n",
      "[Normal] Episode 450/500 — Reward: 30.8, ε: 0.011\n",
      "[Normal] Episode 460/500 — Reward: 31.3, ε: 0.010\n",
      "[Normal] Episode 470/500 — Reward: 28.6, ε: 0.010\n",
      "[Normal] Episode 480/500 — Reward: 34.0, ε: 0.010\n",
      "[Normal] Episode 490/500 — Reward: 31.3, ε: 0.010\n",
      "[Normal] Episode 500/500 — Reward: 30.6, ε: 0.010\n",
      "\n",
      "========================================\n",
      " Training scenario ▶️ Fast\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonkeung/.pyenv/versions/3.11.9/envs/RL2/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fast] Episode 10/500 — Reward: 10.4, ε: 0.904\n",
      "[Fast] Episode 20/500 — Reward: 28.4, ε: 0.818\n",
      "[Fast] Episode 30/500 — Reward: 28.8, ε: 0.740\n",
      "[Fast] Episode 40/500 — Reward: 3.6, ε: 0.669\n",
      "[Fast] Episode 50/500 — Reward: 14.5, ε: 0.605\n",
      "[Fast] Episode 60/500 — Reward: 12.8, ε: 0.547\n",
      "[Fast] Episode 70/500 — Reward: 2.7, ε: 0.495\n",
      "[Fast] Episode 80/500 — Reward: 2.2, ε: 0.448\n",
      "[Fast] Episode 90/500 — Reward: 16.2, ε: 0.405\n",
      "[Fast] Episode 100/500 — Reward: 1.5, ε: 0.366\n",
      "[Fast] Episode 110/500 — Reward: 28.4, ε: 0.331\n",
      "[Fast] Episode 120/500 — Reward: 24.3, ε: 0.299\n",
      "[Fast] Episode 130/500 — Reward: 27.1, ε: 0.271\n",
      "[Fast] Episode 140/500 — Reward: 15.6, ε: 0.245\n",
      "[Fast] Episode 150/500 — Reward: 29.3, ε: 0.221\n",
      "[Fast] Episode 160/500 — Reward: 26.7, ε: 0.200\n",
      "[Fast] Episode 170/500 — Reward: 28.1, ε: 0.181\n",
      "[Fast] Episode 180/500 — Reward: 26.7, ε: 0.164\n",
      "[Fast] Episode 190/500 — Reward: 27.1, ε: 0.148\n",
      "[Fast] Episode 200/500 — Reward: 26.7, ε: 0.134\n",
      "[Fast] Episode 210/500 — Reward: 26.7, ε: 0.121\n",
      "[Fast] Episode 220/500 — Reward: 23.8, ε: 0.110\n",
      "[Fast] Episode 230/500 — Reward: 21.9, ε: 0.099\n",
      "[Fast] Episode 240/500 — Reward: 29.3, ε: 0.090\n",
      "[Fast] Episode 250/500 — Reward: 29.3, ε: 0.081\n",
      "[Fast] Episode 260/500 — Reward: 27.1, ε: 0.073\n",
      "[Fast] Episode 270/500 — Reward: 4.7, ε: 0.066\n",
      "[Fast] Episode 280/500 — Reward: 26.9, ε: 0.060\n",
      "[Fast] Episode 290/500 — Reward: 27.6, ε: 0.054\n",
      "[Fast] Episode 300/500 — Reward: 26.7, ε: 0.049\n",
      "[Fast] Episode 310/500 — Reward: 27.0, ε: 0.044\n",
      "[Fast] Episode 320/500 — Reward: 27.7, ε: 0.040\n",
      "[Fast] Episode 330/500 — Reward: 27.3, ε: 0.036\n",
      "[Fast] Episode 340/500 — Reward: 26.7, ε: 0.033\n",
      "[Fast] Episode 350/500 — Reward: 26.7, ε: 0.030\n",
      "[Fast] Episode 360/500 — Reward: 28.3, ε: 0.027\n",
      "[Fast] Episode 370/500 — Reward: 26.7, ε: 0.024\n",
      "[Fast] Episode 380/500 — Reward: 29.0, ε: 0.022\n",
      "[Fast] Episode 390/500 — Reward: 29.2, ε: 0.020\n",
      "[Fast] Episode 400/500 — Reward: 29.3, ε: 0.018\n",
      "[Fast] Episode 410/500 — Reward: 28.4, ε: 0.016\n",
      "[Fast] Episode 420/500 — Reward: 28.4, ε: 0.015\n",
      "[Fast] Episode 430/500 — Reward: 7.2, ε: 0.013\n",
      "[Fast] Episode 440/500 — Reward: 26.8, ε: 0.012\n",
      "[Fast] Episode 450/500 — Reward: 26.8, ε: 0.011\n",
      "[Fast] Episode 460/500 — Reward: 27.5, ε: 0.010\n",
      "[Fast] Episode 470/500 — Reward: 29.3, ε: 0.010\n",
      "[Fast] Episode 480/500 — Reward: 29.3, ε: 0.010\n",
      "[Fast] Episode 490/500 — Reward: 29.2, ε: 0.010\n",
      "[Fast] Episode 500/500 — Reward: 26.9, ε: 0.010\n",
      "\n",
      "✅ All three scenarios trained for 500 episodes.\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(SCENARIOS)\n",
    "trained = {}\n",
    "for label, env_cfg in SCENARIOS.items():\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\" Training scenario ▶️ {label}\")\n",
    "    print(\"=\"*40)\n",
    "    env, q_model = train_on_env(env_cfg, label)\n",
    "    trained[label] = (env, q_model)\n",
    "\n",
    "print(\"\\nAll three scenarios trained for 500 episodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario  mean_reward  std_reward\n",
      "    Slow    37.030362    6.439829\n",
      "  Normal    28.102651    6.022562\n",
      "    Fast    25.687556    7.172528\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(env, model, episodes=50):\n",
    "    rewards = []\n",
    "    collisions = []\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0.0\n",
    "        collision_count = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < MAX_STEPS_PER_EP:\n",
    "            q_vals = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "            action = int(np.argmax(q_vals))\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            total_reward += reward\n",
    "            # increment collision counter if flagged or via negative reward\n",
    "            if info.get(\"crashed\", False) or reward < 0:\n",
    "                collision_count += 1\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        collisions.append(collision_count)\n",
    "\n",
    "    avg_collisions = np.mean(collisions)\n",
    "    return rewards, avg_collisions\n",
    "\n",
    "# Collect metrics for each scenario\n",
    "metrics = []\n",
    "for label, (env, q_model) in trained.items():\n",
    "    rewards = evaluate_model(env, q_model, episodes=50)\n",
    "    metrics.append({\n",
    "        \"scenario\":     label,\n",
    "        \"mean_reward\":  np.mean(rewards),\n",
    "        \"std_reward\":   np.std(rewards),\n",
    "    })\n",
    "\n",
    "# Display as a table\n",
    "df = pd.DataFrame(metrics)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
